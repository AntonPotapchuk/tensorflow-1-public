{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C2/C2/W2/ungraded_labs/C2_W2_Lab_2_horses_v_humans_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX8mhOLljYeM"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "BZSlp3DAjdYf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1onaG42NZft3wCE1WH0GDEbUhu75fedP5\n",
      "To: C:\\Users\\antpo\\OneDrive\\Desktop\\git\\tensorflow-1-public\\C2\\W2\\ungraded_labs\\horse-or-human.zip\n",
      "\n",
      "  0%|          | 0.00/150M [00:00<?, ?B/s]\n",
      "  1%|          | 1.05M/150M [00:00<00:15, 9.59MB/s]\n",
      "  1%|1         | 2.10M/150M [00:00<00:16, 8.84MB/s]\n",
      "  2%|2         | 3.15M/150M [00:00<00:16, 9.13MB/s]\n",
      "  3%|2         | 4.19M/150M [00:00<00:15, 9.31MB/s]\n",
      "  4%|3         | 5.77M/150M [00:00<00:14, 10.1MB/s]\n",
      "  5%|4         | 7.34M/150M [00:00<00:14, 9.69MB/s]\n",
      "  6%|5         | 8.39M/150M [00:00<00:15, 9.30MB/s]\n",
      "  6%|6         | 9.44M/150M [00:01<00:15, 8.80MB/s]\n",
      "  7%|7         | 10.5M/150M [00:01<00:15, 9.01MB/s]\n",
      "  8%|8         | 12.1M/150M [00:01<00:14, 9.72MB/s]\n",
      "  9%|8         | 13.1M/150M [00:01<00:14, 9.58MB/s]\n",
      " 10%|9         | 14.7M/150M [00:01<00:13, 10.3MB/s]\n",
      " 11%|#         | 15.7M/150M [00:01<00:13, 9.56MB/s]\n",
      " 11%|#1        | 16.8M/150M [00:01<00:13, 9.57MB/s]\n",
      " 12%|#2        | 18.4M/150M [00:01<00:13, 9.77MB/s]\n",
      " 13%|#2        | 19.4M/150M [00:02<00:13, 9.94MB/s]\n",
      " 14%|#4        | 21.0M/150M [00:02<00:12, 10.7MB/s]\n",
      " 15%|#5        | 22.5M/150M [00:02<00:12, 9.79MB/s]\n",
      " 16%|#5        | 23.6M/150M [00:02<00:13, 9.65MB/s]\n",
      " 17%|#6        | 25.2M/150M [00:02<00:12, 10.2MB/s]\n",
      " 18%|#7        | 26.2M/150M [00:02<00:12, 9.55MB/s]\n",
      " 18%|#8        | 27.3M/150M [00:02<00:12, 9.56MB/s]\n",
      " 19%|#8        | 28.3M/150M [00:02<00:13, 9.29MB/s]\n",
      " 20%|#9        | 29.9M/150M [00:03<00:12, 9.86MB/s]\n",
      " 21%|##1       | 31.5M/150M [00:03<00:11, 9.91MB/s]\n",
      " 22%|##2       | 33.0M/150M [00:03<00:11, 10.4MB/s]\n",
      " 23%|##3       | 34.6M/150M [00:03<00:11, 10.2MB/s]\n",
      " 24%|##3       | 35.7M/150M [00:03<00:11, 10.1MB/s]\n",
      " 25%|##4       | 37.2M/150M [00:03<00:10, 10.7MB/s]\n",
      " 26%|##5       | 38.8M/150M [00:03<00:10, 10.7MB/s]\n",
      " 27%|##6       | 40.4M/150M [00:04<00:10, 10.9MB/s]\n",
      " 28%|##8       | 41.9M/150M [00:04<00:09, 11.0MB/s]\n",
      " 29%|##9       | 43.5M/150M [00:04<00:12, 8.35MB/s]\n",
      " 30%|##9       | 44.6M/150M [00:04<00:12, 8.44MB/s]\n",
      " 31%|###       | 46.1M/150M [00:04<00:11, 9.00MB/s]\n",
      " 32%|###1      | 47.2M/150M [00:04<00:11, 9.28MB/s]\n",
      " 32%|###2      | 48.2M/150M [00:04<00:11, 9.17MB/s]\n",
      " 33%|###3      | 49.8M/150M [00:05<00:10, 9.79MB/s]\n",
      " 34%|###4      | 50.9M/150M [00:05<00:10, 9.86MB/s]\n",
      " 35%|###5      | 52.4M/150M [00:05<00:09, 10.6MB/s]\n",
      " 36%|###6      | 54.0M/150M [00:05<00:09, 10.1MB/s]\n",
      " 37%|###6      | 55.1M/150M [00:05<00:09, 9.73MB/s]\n",
      " 38%|###7      | 56.1M/150M [00:05<00:09, 9.69MB/s]\n",
      " 38%|###8      | 57.1M/150M [00:05<00:09, 9.58MB/s]\n",
      " 39%|###9      | 58.7M/150M [00:06<00:09, 9.83MB/s]\n",
      " 40%|###9      | 59.8M/150M [00:06<00:09, 9.41MB/s]\n",
      " 41%|####      | 60.8M/150M [00:06<00:10, 8.79MB/s]\n",
      " 42%|####1     | 62.4M/150M [00:06<00:09, 9.53MB/s]\n",
      " 43%|####2     | 64.0M/150M [00:06<00:08, 10.0MB/s]\n",
      " 43%|####3     | 65.0M/150M [00:06<00:08, 9.93MB/s]\n",
      " 44%|####4     | 66.1M/150M [00:06<00:08, 9.47MB/s]\n",
      " 45%|####5     | 67.6M/150M [00:06<00:07, 10.3MB/s]\n",
      " 46%|####6     | 69.2M/150M [00:07<00:07, 10.6MB/s]\n",
      " 47%|####7     | 70.8M/150M [00:07<00:07, 10.4MB/s]\n",
      " 48%|####8     | 71.8M/150M [00:07<00:07, 10.4MB/s]\n",
      " 49%|####9     | 73.4M/150M [00:07<00:07, 10.5MB/s]\n",
      " 50%|#####     | 75.0M/150M [00:07<00:06, 10.8MB/s]\n",
      " 51%|#####1    | 76.5M/150M [00:07<00:06, 10.6MB/s]\n",
      " 52%|#####2    | 78.1M/150M [00:07<00:06, 10.3MB/s]\n",
      " 53%|#####3    | 79.7M/150M [00:08<00:06, 10.6MB/s]\n",
      " 54%|#####4    | 81.3M/150M [00:08<00:06, 10.4MB/s]\n",
      " 55%|#####5    | 82.8M/150M [00:08<00:06, 10.6MB/s]\n",
      " 56%|#####6    | 84.4M/150M [00:08<00:06, 10.5MB/s]\n",
      " 57%|#####7    | 85.5M/150M [00:08<00:06, 10.1MB/s]\n",
      " 58%|#####8    | 87.0M/150M [00:08<00:06, 10.1MB/s]\n",
      " 59%|#####9    | 88.6M/150M [00:08<00:05, 10.4MB/s]\n",
      " 60%|######    | 90.2M/150M [00:09<00:05, 10.7MB/s]\n",
      " 61%|######1   | 91.8M/150M [00:09<00:05, 10.9MB/s]\n",
      " 62%|######2   | 93.3M/150M [00:09<00:05, 10.6MB/s]\n",
      " 63%|######3   | 94.9M/150M [00:09<00:05, 9.53MB/s]\n",
      " 64%|######4   | 95.9M/150M [00:09<00:06, 8.20MB/s]\n",
      " 65%|######4   | 97.0M/150M [00:09<00:06, 8.24MB/s]\n",
      " 66%|######5   | 98.0M/150M [00:10<00:05, 8.60MB/s]\n",
      " 67%|######6   | 99.6M/150M [00:10<00:05, 9.69MB/s]\n",
      " 67%|######7   | 101M/150M [00:10<00:05, 9.66MB/s] \n",
      " 68%|######8   | 102M/150M [00:10<00:04, 10.5MB/s]\n",
      " 69%|######9   | 104M/150M [00:10<00:04, 10.3MB/s]\n",
      " 70%|#######   | 105M/150M [00:10<00:04, 9.79MB/s]\n",
      " 71%|#######1  | 106M/150M [00:10<00:04, 10.2MB/s]\n",
      " 72%|#######1  | 107M/150M [00:10<00:04, 10.1MB/s]\n",
      " 73%|#######2  | 109M/150M [00:10<00:04, 10.2MB/s]\n",
      " 73%|#######3  | 110M/150M [00:11<00:04, 9.61MB/s]\n",
      " 74%|#######3  | 111M/150M [00:11<00:04, 9.13MB/s]\n",
      " 75%|#######5  | 112M/150M [00:11<00:03, 9.46MB/s]\n",
      " 76%|#######6  | 114M/150M [00:11<00:03, 9.76MB/s]\n",
      " 77%|#######6  | 115M/150M [00:11<00:03, 9.63MB/s]\n",
      " 77%|#######7  | 116M/150M [00:11<00:03, 9.62MB/s]\n",
      " 78%|#######8  | 117M/150M [00:11<00:03, 9.48MB/s]\n",
      " 79%|#######8  | 118M/150M [00:12<00:03, 9.51MB/s]\n",
      " 80%|#######9  | 120M/150M [00:12<00:03, 9.87MB/s]\n",
      " 81%|########  | 121M/150M [00:12<00:02, 9.79MB/s]\n",
      " 82%|########1 | 122M/150M [00:12<00:02, 9.79MB/s]\n",
      " 82%|########2 | 123M/150M [00:12<00:02, 9.39MB/s]\n",
      " 83%|########3 | 125M/150M [00:12<00:02, 9.96MB/s]\n",
      " 84%|########4 | 126M/150M [00:12<00:02, 10.3MB/s]\n",
      " 85%|########5 | 127M/150M [00:12<00:02, 10.4MB/s]\n",
      " 86%|########6 | 129M/150M [00:13<00:01, 10.6MB/s]\n",
      " 87%|########7 | 131M/150M [00:13<00:01, 10.4MB/s]\n",
      " 88%|########8 | 132M/150M [00:13<00:01, 10.7MB/s]\n",
      " 89%|########9 | 134M/150M [00:13<00:01, 10.8MB/s]\n",
      " 90%|######### | 135M/150M [00:13<00:01, 10.7MB/s]\n",
      " 91%|#########1| 137M/150M [00:13<00:01, 10.5MB/s]\n",
      " 93%|#########2| 138M/150M [00:13<00:01, 10.0MB/s]\n",
      " 93%|#########3| 139M/150M [00:14<00:01, 9.80MB/s]\n",
      " 94%|#########3| 141M/150M [00:14<00:00, 9.96MB/s]\n",
      " 95%|#########4| 142M/150M [00:14<00:00, 9.84MB/s]\n",
      " 95%|#########5| 143M/150M [00:14<00:00, 9.70MB/s]\n",
      " 96%|#########6| 144M/150M [00:14<00:00, 10.3MB/s]\n",
      " 97%|#########7| 145M/150M [00:14<00:00, 9.70MB/s]\n",
      " 98%|#########7| 146M/150M [00:14<00:00, 7.95MB/s]\n",
      " 98%|#########8| 147M/150M [00:15<00:00, 7.78MB/s]\n",
      " 99%|#########9| 148M/150M [00:15<00:00, 7.93MB/s]\n",
      "100%|##########| 150M/150M [00:15<00:00, 8.70MB/s]\n",
      "100%|##########| 150M/150M [00:15<00:00, 9.80MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LYeusSEIiZQpwN-mthh5nKdA75VsKG1U\n",
      "To: C:\\Users\\antpo\\OneDrive\\Desktop\\git\\tensorflow-1-public\\C2\\W2\\ungraded_labs\\validation-horse-or-human.zip\n",
      "\n",
      "  0%|          | 0.00/11.5M [00:00<?, ?B/s]\n",
      " 14%|#3        | 1.57M/11.5M [00:00<00:01, 9.56MB/s]\n",
      " 23%|##2       | 2.62M/11.5M [00:00<00:01, 8.41MB/s]\n",
      " 32%|###1      | 3.67M/11.5M [00:00<00:00, 9.06MB/s]\n",
      " 46%|####5     | 5.24M/11.5M [00:00<00:00, 10.1MB/s]\n",
      " 55%|#####4    | 6.29M/11.5M [00:00<00:00, 9.33MB/s]\n",
      " 64%|######3   | 7.34M/11.5M [00:00<00:00, 9.52MB/s]\n",
      " 78%|#######7  | 8.91M/11.5M [00:00<00:00, 9.92MB/s]\n",
      " 91%|#########1| 10.5M/11.5M [00:01<00:00, 10.4MB/s]\n",
      "100%|##########| 11.5M/11.5M [00:01<00:00, 9.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "# horses-or-humans dataset\n",
    "!gdown --id 1onaG42NZft3wCE1WH0GDEbUhu75fedP5\n",
    "\n",
    "# horses-or-humans validation dataset\n",
    "!gdown --id 1LYeusSEIiZQpwN-mthh5nKdA75VsKG1U\n",
    "  \n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile('./horse-or-human.zip', 'r')\n",
    "zip_ref.extractall('tmp/horse-or-human')\n",
    "\n",
    "zip_ref = zipfile.ZipFile('./validation-horse-or-human.zip', 'r')\n",
    "zip_ref.extractall('tmp/validation-horse-or-human')\n",
    "\n",
    "zip_ref.close()\n",
    "\n",
    "# Directory with our training horse pictures\n",
    "train_horse_dir = os.path.join('tmp/horse-or-human/horses')\n",
    "\n",
    "# Directory with our training human pictures\n",
    "train_human_dir = os.path.join('tmp/horse-or-human/humans')\n",
    "\n",
    "# Directory with our training horse pictures\n",
    "validation_horse_dir = os.path.join('tmp/validation-horse-or-human/horses')\n",
    "\n",
    "# Directory with our training human pictures\n",
    "validation_human_dir = os.path.join('tmp/validation-horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Building a Small Model from Scratch\n",
    "\n",
    "But before we continue, let's start defining the model:\n",
    "\n",
    "Step 1 will be to import tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qvfZg3LQbD-5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnhYCP4tdqjC"
   },
   "source": [
    "We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gokG5HKpdtzm"
   },
   "source": [
    "Finally we add the densely connected layers. \n",
    "\n",
    "Note that because we are facing a two-class classification problem, i.e. a *binary classification problem*, we will end our network with a [*sigmoid* activation](https://wikipedia.org/wiki/Sigmoid_function), so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PixZ2s5QbYQ3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'tmp/horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(300, 300),  # All images will be resized to 150x150\n",
    "        batch_size=128,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(300, 300),  # All images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=100,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zNPRWOVJdOH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "C2_W2_Lab_2_horses_v_humans_augmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow-course",
   "language": "python",
   "name": "tensorflow-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
